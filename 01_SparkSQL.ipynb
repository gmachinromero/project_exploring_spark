{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a1fc2d9-7eb9-45e0-ae7d-1946a105504f",
   "metadata": {},
   "source": [
    "## 1. Primeros pasos con SparkSQL\n",
    "\n",
    "Spark SQL es un módulo de Spark para el procesamiento de datos estructurados. A diferencia de la API de Spark RDD, las interfaces proporcionadas por Spark SQL brindan a Spark más información sobre la estructura tanto de los datos, como de los cálculos que se están realizando.\n",
    "\n",
    "Internamente, Spark SQL usa esta información extra para realizar optimizaciones adicionales. Hay varias formas de interactuar con Spark SQL, incluidas SQL y la DataFrame API. Al calcular un resultado, se utiliza el mismo motor de ejecución, independientemente de qué API / lenguaje esté utilizando para expresar el cálculo. Esta unificación significa que los desarrolladores pueden alternar fácilmente entre diferentes API en función de cuál proporciona la forma más natural de expresar unas transformaciones determinadas.\n",
    "\n",
    "SparkSQL se sustenta sobre el concepto de DataFrame, la cual es una colección de datos tabular, que al igual que los RDDs, se encuentra distribuida en los diferentes nodos de un cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64cfabf-88c1-4334-bde2-ddb403e46933",
   "metadata": {},
   "source": [
    "### 1.1. SQLContext\n",
    "\n",
    "El punto de entrada a las funcionalidades de SparkSQL es la clase SparkSesion. Para crear una SparkSession básica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "feed5c49-5867-4d3a-8383-b370f907e87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías y dependencias\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c0012dc-7b4b-4970-ab4a-18c34dca8d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/20 10:28:42 WARN Utils: Your hostname, gmachin resolves to a loopback address: 127.0.1.1; using 192.168.0.19 instead (on interface wlp3s0)\n",
      "22/05/20 10:28:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark-3.2.0/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/20 10:28:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/05/20 10:28:45 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.app.name\", \"myApp\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bcb34e7-9400-4dea-8642-cb9e0f15583d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.19:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>myApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f82f40ad070>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d137c3c-78f9-4597-98d9-b742fe58b5f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.name', 'myApp'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.host', '192.168.0.19'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/home/gmachin/git/project_exploring_spark/spark-warehouse'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.driver.port', '38563'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.app.id', 'local-1653035325796'),\n",
       " ('spark.app.startTime', '1653035323512')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b664666b-498f-4e61-b07c-739f5392e69d",
   "metadata": {},
   "source": [
    "### 1.2. DataFrame\n",
    "\n",
    "Se puede crear un DataFrame a partir de un RDD, una tabla de Hive, o otro tipo de datos de Python como listas o DataFrames de Pandas. Para crear un DataFrame podemos utilizas el método `spark.createDataFrame(data)`, o leyendo datos externos con `spark.read.csv()`, `spark.read.json()`...etc.\n",
    "\n",
    "Orígenes posibles de datos:\n",
    "\n",
    "- RDDs\n",
    "- Hive\n",
    "- Spark sources: parquet (defecto), json, jdbc, orc, libsvm, csv, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ed1f0f1a-4a93-4b19-92bf-2057c24f2f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'naked mole rat'),\n",
       " (1, 'capibara'),\n",
       " (2, 'axolotl'),\n",
       " (3, 'capibara'),\n",
       " (4, 'axolotl')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear una lista de tuplas\n",
    "import random\n",
    "\n",
    "random.seed(7)\n",
    "ids = range(5)\n",
    "species = random.choices(['capibara', 'naked mole rat', 'axolotl', 'flying fox'], k=5)\n",
    "\n",
    "data = list(zip(ids, species))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cab0045a-6603-4560-b0a3-3a80e8b936e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[uuid: int, name: string]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear un schema\n",
    "schema = StructType([\n",
    "    StructField('uuid', IntegerType(), True),\n",
    "    StructField('name', StringType(), True)\n",
    "])\n",
    "\n",
    "# Crear un DataFrame a partir de los datos anteriores\n",
    "df = spark.createDataFrame(\n",
    "    data=data,\n",
    "    schema=schema,\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a291104a-351c-42cd-8449-99f644a6abff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uuid: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ver el esquema de datos en formato árbol\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0607393b-6330-45d7-a299-1f8472ee23ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(uuid=0, name='naked mole rat'),\n",
       " Row(uuid=1, name='capibara'),\n",
       " Row(uuid=2, name='axolotl'),\n",
       " Row(uuid=3, name='capibara'),\n",
       " Row(uuid=4, name='axolotl')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "190f5035-ffa3-4dcc-9489-66bc8e3cdde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+\n",
      "|uuid|          name|\n",
      "+----+--------------+\n",
      "|   0|naked mole rat|\n",
      "|   1|      capibara|\n",
      "|   2|       axolotl|\n",
      "|   3|      capibara|\n",
      "|   4|       axolotl|\n",
      "+----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualizar el contenido del DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53fe119-01d9-4ae6-9636-494998a38308",
   "metadata": {},
   "source": [
    "En este caso, vamos a leer datos de un fichero externo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7946b3b9-15df-4f74-9ae6-954a32aeccb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_quijote = spark.read.text(\"ficheros/extracto-quijote.txt\")\n",
    "df_quijote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd8ccdb7-d9f4-4d06-9cf3-7b35743ff6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_quijote.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9d745a4-bb76-4232-a1d0-7fb2268ed25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|En un lugar de la...|\n",
      "|No ha mucho tiemp...|\n",
      "|astillero, adarga...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_quijote.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f46e8275-53f0-44aa-80b3-b1f4fea97db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe :\n",
      "+----+---+------+\n",
      "|Name|Age|Gender|\n",
      "+----+---+------+\n",
      "+----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crear un RDD vacío\n",
    "emp_RDD = spark.sparkContext.emptyRDD()\n",
    " \n",
    "# Crear un schema\n",
    "columns = StructType([\n",
    "    StructField('Name', StringType(), True),\n",
    "    StructField('Age', StringType(), True),\n",
    "    StructField('Gender', StringType(), True)\n",
    "])\n",
    " \n",
    "# Aplicar schema al RDD\n",
    "df = spark.createDataFrame(\n",
    "    data = emp_RDD,\n",
    "    schema = columns\n",
    ")\n",
    " \n",
    "# Mostrar DataFrame\n",
    "print('Dataframe :')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b215740c-0387-4c08-b3ea-d744a1d24851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema :\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostrar Esquema\n",
    "print('Schema :')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cce776a-8d92-44cf-b243-579bbf952375",
   "metadata": {},
   "source": [
    "### 1.3. Operaciones con DataFrames\n",
    "\n",
    "En Pyhton, es posible acceder a las diferentes columnas de los DataFrames, bien por atributos (`df.age`), o bien por índices (`df['age']`). Mientras que la primera opción es más conveniente para un análisis exploratorio e interactivo, se recomienda utilizar la segunda forma, que está preparada para el futuro y no se romperá con los nombres de columna que también son atributos de la clase DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0adb8a6b-6b71-4c94-9a08-eebce2a72f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "None\n",
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creamos un DataFrame a partir de un fichero json\n",
    "df = spark.read.json(\"ficheros/people.json\")\n",
    "\n",
    "print(df.printSchema())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b9d0671b-dcd7-46f6-9bf7-d66418be59fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|   Andy|\n",
      "| Justin|\n",
      "|Michael|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Seleccionar la columna name\n",
    "df.select(df['name']).orderBy('name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "918a7cae-6eb7-4b1a-9879-fabf5342c2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtrar aquellos registros cuya edad sea superior a 21\n",
    "df.filter(df['age'] > 21).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c42b6ce-d91e-487e-a4d3-aaabdc181e41",
   "metadata": {},
   "source": [
    "Además de los métodos proporcionados por la clase DataFrame, Spark permite mediante la función sql de la SparkSession ejecutar querys de tipo SQL de forma programática. Para ello necesitaremos registrar el DataFrame que queramos explorar en la SparkSession para que sea accesible por el motor SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ad258a7-9951-4b69-b11a-58962a5f7eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Registrar el DtaFrame como una vista temporal\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "sql_df = spark.sql(\n",
    "    \"\"\"\n",
    "    select *\n",
    "    from people\n",
    "    where age is not null\n",
    "      and name like 'A%'\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "sql_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ae7c4f-608c-4f5e-8686-6dd3e3d5fc27",
   "metadata": {},
   "source": [
    "## 2. Bibliografía\n",
    "\n",
    "- https://spark.apache.org/docs/latest/rdd-programming-guide.html\n",
    "- https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "- https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#dataframe-apis\n",
    "- Libro: Introducción a Apache Spark "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
