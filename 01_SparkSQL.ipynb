{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a1fc2d9-7eb9-45e0-ae7d-1946a105504f",
   "metadata": {},
   "source": [
    "## 1. Primeros pasos con SparkSQL\n",
    "\n",
    "Spark SQL es un módulo de Spark para el procesamiento de datos estructurados. A diferencia de la API de Spark RDD, las interfaces proporcionadas por Spark SQL brindan a Spark más información sobre la estructura tanto de los datos, como de los cálculos que se están realizando.\n",
    "\n",
    "Internamente, Spark SQL usa esta información extra para realizar optimizaciones adicionales. Hay varias formas de interactuar con Spark SQL, incluidas SQL y la Dataframe API. Al calcular un resultado, se utiliza el mismo motor de ejecución, independientemente de qué API / lenguaje esté utilizando para expresar el cálculo. Esta unificación significa que los desarrolladores pueden alternar fácilmente entre diferentes API en función de cuál proporciona la forma más natural de expresar unas transformaciones determinadas.\n",
    "\n",
    "SparkSQL se sustenta sobre el concepto de Dataframe, la cual es una colección de datos tabular, que al igual que los RDDs, se encuentra distribuida en los diferentes nodos de un cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64cfabf-88c1-4334-bde2-ddb403e46933",
   "metadata": {},
   "source": [
    "### 1.1. SQLContext\n",
    "\n",
    "El punto de entrada a las funcionalidades de SparkSQL es la clase SparkSesion. Para crear una SparkSession básica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feed5c49-5867-4d3a-8383-b370f907e87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías y dependencias\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c0012dc-7b4b-4970-ab4a-18c34dca8d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "                    .master(\"local[*]\")\\\n",
    "                    .config(\"spark.app.name\", \"myApp\")\\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bcb34e7-9400-4dea-8642-cb9e0f15583d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.43:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f240410eeb0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d137c3c-78f9-4597-98d9-b742fe58b5f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.startTime', '1637406307040'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/home/gmachin/git/project_exploring_spark/spark-warehouse'),\n",
       " ('spark.driver.host', '192.168.1.43'),\n",
       " ('spark.driver.port', '44009'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.app.id', 'local-1637406308066'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.app.name', 'pyspark-shell')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b664666b-498f-4e61-b07c-739f5392e69d",
   "metadata": {},
   "source": [
    "### 1.2. Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed1f0f1a-4a93-4b19-92bf-2057c24f2f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'naked mole rat'),\n",
       " (1, 'capibara'),\n",
       " (2, 'axolotl'),\n",
       " (3, 'capibara'),\n",
       " (4, 'axolotl')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(7)\n",
    "ids = range(5)\n",
    "species = random.choices(['capibara', 'naked mole rat', 'axolotl', 'flying fox'], k=5)\n",
    "\n",
    "data = list(zip(ids, species))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cab0045a-6603-4560-b0a3-3a80e8b936e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: bigint, _2: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0607393b-6330-45d7-a299-1f8472ee23ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(_1=0, _2='naked mole rat'),\n",
       " Row(_1=1, _2='capibara'),\n",
       " Row(_1=2, _2='axolotl'),\n",
       " Row(_1=3, _2='capibara'),\n",
       " Row(_1=4, _2='axolotl')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "190f5035-ffa3-4dcc-9489-66bc8e3cdde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "| _1|            _2|\n",
      "+---+--------------+\n",
      "|  0|naked mole rat|\n",
      "|  1|      capibara|\n",
      "|  2|       axolotl|\n",
      "|  3|      capibara|\n",
      "|  4|       axolotl|\n",
      "+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ae7c4f-608c-4f5e-8686-6dd3e3d5fc27",
   "metadata": {},
   "source": [
    "## 2. Bibliografía\n",
    "\n",
    "- https://spark.apache.org/docs/latest/rdd-programming-guide.html\n",
    "- https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "- Libro: Introducción a Apache Spark "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
