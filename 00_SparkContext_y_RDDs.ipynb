{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05efd63c-9661-45cb-baed-50b8705e5c1c",
   "metadata": {},
   "source": [
    "## 1. Primeros pasos con Spark\n",
    "\n",
    "En este notebook vamos a ver una breve introducción a Spark con Python, y a una de sus abstracciones má comunes, los RDDs, conjuntos de datos distribuidos en un cluster. Para ello vamos a utilizar la librería PySpark, que es la API de Python para interactuar con la shell de Spark. Se puede instalar la librería mediante la siguiente guía:\n",
    "\n",
    "- https://www.sicara.ai/blog/2017-05-02-get-started-pyspark-jupyter-notebook-3-minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a52afba-ee27-4045-b27c-56e6f8b71cba",
   "metadata": {},
   "source": [
    "### 1.1. SparkContext\n",
    "\n",
    "El punto de entrada de cualquier programa Spark es un objeto SparkContext, este objeto le indica como conectarse a un clúster de Spark y crear RDDs. Para crer un SparkContext, primero es necesario cosntruir un objeto SparkConf conteniendo la información de la aplicación.\n",
    "\n",
    "```Python\n",
    "conf = SparkConf().setAppName('myApp').setMaster('local[*]')\n",
    "sc = SparkContext(conf=conf)\n",
    "```\n",
    "\n",
    "El parámetro `local[*]` es una cadena especial que indica que está utilizando un clúster local. El asterisco * le indica a Spark que cree tantos hilos de trabajo como cores tenga la máquina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71556942-9459-415a-b3a9-fd1ace9f6675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías y dependencias\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2cefb35-ee10-4bcc-b09a-2aa4a7c5922f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/20 09:59:34 WARN Utils: Your hostname, gmachin resolves to a loopback address: 127.0.1.1; using 192.168.0.19 instead (on interface wlp3s0)\n",
      "22/05/20 09:59:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark-3.2.0/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/20 09:59:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# SparkContext\n",
    "conf = SparkConf().setAppName('myApp').setMaster('local[*]')\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "345aeeb0-232b-48c9-836f-7f5a763eba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.setLogLevel('WARN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ac5db6-129e-491d-bb64-4cf88b1fbeda",
   "metadata": {},
   "source": [
    "Con este objeto, podemos acceder a la SparkUI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65ad7b63-6a24-4082-891c-8c92a4902052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.19:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>myApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=myApp>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb21526e-ac63-40a9-811f-30c955a3da66",
   "metadata": {},
   "source": [
    "Y consultar el resto de configuraciones por defecto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44df61b9-fd70-4370-aba4-84009ec285d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.id', 'local-1653033576696'),\n",
       " ('spark.driver.port', '36439'),\n",
       " ('spark.app.name', 'myApp'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.host', '192.168.0.19'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.app.startTime', '1653033575227')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2641d514-ecae-4cd3-b2a9-75577e2efac3",
   "metadata": {},
   "source": [
    "### 1.2. RDD\n",
    "\n",
    "Spark gira en torno al concepto de *Resilient Distributed Dataset* (RDD), que es una colección de elementos distribuidos en el cluster y que se pueden operar en paralelo. Hay dos formas de crear un RDD: paralelizar una colección existente, o hacer referencia a un conjunto de datos en un sistema de almacenamiento externo, como un sistema de archivos compartido, HDFS, HBase o cualquier fuente de datos que ofrezca un formato de entrada Hadoop.\n",
    "\n",
    "Para crear un RDD se utiliza el método `parallelize` del objeto SparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1cc06ae-e2f1-4195-945f-b3682abb80ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear un RDD a partir de una colección de Python\n",
    "rdd = sc.parallelize(range(10000000))\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "344297ee-73b7-4279-9991-76f34abb023d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ficheros/extracto-quijote.txt MapPartitionsRDD[3] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear un RDD a partir de un fichero externo\n",
    "rdd_quijote = sc.textFile(\"ficheros/extracto-quijote.txt\")\n",
    "rdd_quijote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d70d865-4308-4a15-b66b-15609d35e30f",
   "metadata": {},
   "source": [
    "Un parámetro importante para los RDD es el número de particiones para distribuir el conjunto de datos. Spark ejecutará una task para cada partición del clúster. Por lo general, se utilizan de 2 a 4 particiones para cada CPU del clúster. Normalmente, Spark infiere la cantidad de particiones automáticamente en función de los recursos. Sin embargo, también puede configurarlo manualmente pasándolo como un segundo parámetro de parallelize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fed3ba8e-0c63-4939-90b1-c42d70851443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ver las particiones de un RDD (valor 4 por defecto)\n",
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1351f23f-fedb-4d16-a920-3772a011e63d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_quijote.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d3e9e75-21e7-48dc-855a-d41cbba6b953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reparticionar un RDD\n",
    "rdd_repartitioned = rdd.repartition(8)\n",
    "rdd_repartitioned.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71564235-4e67-40c7-aae1-a985c234c84f",
   "metadata": {},
   "source": [
    "### 1.3. Operaciones con RDD\n",
    "\n",
    "Spark permite dos tipos de operaciones sobre los RDD's:\n",
    "\n",
    "- transformaciones\n",
    "- acciones\n",
    "\n",
    "Las transformaciones tienen como consecuencia la creación de un nuevo dataset a partir de uno ya existente, mientras que las acciones, devuelven un valor al driver del cluster después de realizar alguna operación sobre el dataset. La función `map()` por ejemplo sería una transformación, ya que aplica una función a cada valor de un dataset; por otro lado, la función `reduce()` devuelve un único valor fruto de algún tipo de agregación.\n",
    "\n",
    "Las transformaciones en Spark se denominan *lazy*, ya que no se ejecutan hasta que se aplica una acción sobre el dataset. Esta característica permite que Spark se ejecute de una forma más eficiente. Por ejemplo si sobre un dataset realizamos un `map()` y luego un `reduce()`, al driver únicamente se le informará del valor de la acción y no de todos los pasos anteriores.\n",
    "\n",
    "Algunas de las operaciones más usadas son:\n",
    "\n",
    "- transformaciones: map, filter, reduceByKey\n",
    "- acciones: reduce, collect, take, count\n",
    "\n",
    "Las transformaciones y acciones definen lo que se conoce como un DAG (Directed Acyclic Graph), el cual represnta el job de Spark a ejecutar.\n",
    "\n",
    "Aunque sc.parallelize y sc.textFile técnicamente no son transformaciones, podemos pensar en ellas como si lo fueran ya que son *lazy*. Si intentamos ver el contenido de lineLengths, veremos que no aparece nada, y solo da información del tipo de objeto que es, aún no se ha realizado ninguna operación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07d33344-d6c8-44b7-bc66-80ab6d15b3da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[13] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo de operaciones con un RDD\n",
    "# ------------------------------------------------------------------------------\n",
    "# Definimos un puntero al fichero\n",
    "lines = sc.textFile(\"ficheros/extracto-quijote.txt\")\n",
    "\n",
    "# Definimos un nuevo RDD sobre el puntero anterior\n",
    "lineLengths = lines.map(lambda s: len(s))\n",
    "\n",
    "lineLengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29ed254-c6ec-4a32-884b-0357f66306db",
   "metadata": {},
   "source": [
    "Finalmente, realizamos una acción sobre el RDD lineLengths. Es en este punto, donde Spark comienza a ejecutar todo el trabajo sobre el RDD, para finalmente enviar un valor al driver del cluster. Las acciones son las que hacen que e motor de Spark genere el trabajo, y es cuando se pueden observas los jobs, stages y tasks en la SparkUI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2f4e631-3b28-4965-8856-973d89a95a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Realizamos una acción sobre el RDD\n",
    "totalLength = lineLengths.reduce(lambda a, b: a + b)\n",
    "\n",
    "totalLength"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c92ba6-d331-47cb-8336-fe9642128da1",
   "metadata": {},
   "source": [
    "Para demostrar de nuevo que la creación o transformación de un RDD son operaciones *lazy*, vamos a intentar visualizar los datos de un RDD, lo cual es imposible, a no ser que se utilice la acción `collect()`, la cual recoge todas las particiones del RDD y las convierte en una lista de Python.\n",
    "\n",
    "Al intentar imprimir por pantalla el rdd lines, vemos que solo nos devuelve información de a donde apunta le objeto lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e0c34a3-f170-4c23-aaff-f5b5759acbde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ficheros/extracto-quijote.txt MapPartitionsRDD[10] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apuntar a un fichero de texto\n",
    "lines = sc.textFile(\"ficheros/extracto-quijote.txt\")\n",
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5467db8b-9834-4287-8563-b8592c55fa54",
   "metadata": {},
   "source": [
    "Al utilizar `collect()` sobre el RDD anterior, comprobamos que podemos imprimirlo por pantalla, y que se trata de una lista de Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b05bc62a-27fc-4e41-82b3-292a3a3d5c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['En un lugar de la Mancha, de cuyo nombre no quiero acordarme, ',\n",
       " 'No ha mucho tiempo que vivía un hidalgo de los de lanza en',\n",
       " 'astillero, adarga antigua, rocín flaco y galgo corredor.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostrar el contenido del RDD\n",
    "lines.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d32e654-1196-4b06-b6c6-41977e4fd540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ver el tipo de objeto\n",
    "type(lines.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d05b642-b545-47f3-ba12-cd7ab6def38e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['En un lugar de la Mancha, de cuyo nombre no quiero acordarme, ']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Slice de una lista\n",
    "lines.collect()[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7c0c4f-0751-4a57-b04e-853227672561",
   "metadata": {},
   "source": [
    "Existen tres conceptos que están muy ligados a las operaciones que hemos visto en este punto, y que son los jobs, stages y tasks. Cada vez que se ejecuta una acción el motor de Spark ejecuta las operaciones programadas, dividiéndose estas en:\n",
    "\n",
    "- Jobs: Trabajos de Spark que terminan en una acción\n",
    "- Stages: Todas las parte de un job que se pueden ejecutar en una sola partición de Spark\n",
    "- Tasks: Se realizan tantas Tasks como particiones haya en cada operación de un Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b5cad9-fb63-4d90-83c3-ecbd6d778a82",
   "metadata": {},
   "source": [
    "### 1.4. Persistencia de RDD\n",
    "\n",
    "Como hemos visto en el punto anterior, las tranformaciones sobre un RDD son *lazy*, esto significa que Spark almacena como transformar cada RDD pero no ejecuta estas transformaciones hasta que se realiza una acción sobre uno de estos. Lo que en primera instancia podría ser una ventaja para evitar que la memoria de Spark llegue al límite, puede convertirse en un problema si tenemos que realizar repetidas acciones sobre un mismo RDD ya transformado, ya que acada vez que se quiera realizar una acción, deberán realizarse también todas las transformaciones anteriores, y cuando se trabaja con ficheros muy grandes, puede resultar costoso.\n",
    "\n",
    "Para resolver este inconveniente, Spark cuenta con dos métodos, `persist()` y `cache()`, los cuales permiten almacenar en memoria un RDD sobre el que se vayan a realizar acciones de forma repetida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cede6e2d-090e-4e62-b4fc-a5bbf2b9507e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[13] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "# Persist por defecto se realiza en memoria, pero pueden configurarse otras opciones\n",
    "lineLengths.persist() # Equivale a lineLengths.persist(StorageLevel.ONLY_MEMORY)\n",
    "lineLengths.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1162ac78-4586-43ce-abed-c17791b1d795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[13] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lineLengths.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d739a9-ea57-4a10-9cdc-d12535e31b67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15ddead2-6276-4cb6-85f9-32c4e10ce75e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85f32295-248e-4c49-8f74-99101e1a3cd3",
   "metadata": {},
   "source": [
    "## 2. Bibliografía\n",
    "\n",
    "- https://spark.apache.org/docs/latest/rdd-programming-guide.html\n",
    "- Libro: Introducción a Apache Spark "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
