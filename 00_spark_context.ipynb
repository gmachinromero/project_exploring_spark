{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05efd63c-9661-45cb-baed-50b8705e5c1c",
   "metadata": {},
   "source": [
    "## 1. Primeros pasos con Spark\n",
    "\n",
    "Vamos a utilizar la librería PySpark, que es la API de Python para interactuar con la shell de Spark. Se puede instalar la librería mediante la siguiente guía:\n",
    "\n",
    "- https://www.sicara.ai/blog/2017-05-02-get-started-pyspark-jupyter-notebook-3-minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a326aed-85a7-48d6-aa2a-8ce5e8dfa435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías y dependencias\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a52afba-ee27-4045-b27c-56e6f8b71cba",
   "metadata": {},
   "source": [
    "### 1.1. SparkContext\n",
    "\n",
    "El punto de entrada de cualquier programa PySpark es un objeto SparkContext, este objeto le indica como conectarse a un clúster de Spark y crear RDDs. Para crer un SparkContext, primero es necesario cosntruir un objeto SparkConf conteniendo la información de la aplicación.\n",
    "\n",
    "```Python\n",
    "conf = SparkConf().setAppName('myApp').setMaster('local[*]')\n",
    "sc = SparkContext(conf=conf)\n",
    "```\n",
    "\n",
    "El parámetro `local[*]` es una cadena especial que indica que está utilizando un clúster local. El asterisco * le indica a Spark que cree tantos hilos de trabajo como cores tenga la máquina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2cefb35-ee10-4bcc-b09a-2aa4a7c5922f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/18 19:21:09 WARN Utils: Your hostname, gmachin resolves to a loopback address: 127.0.1.1; using 192.168.1.43 instead (on interface enp4s0)\n",
      "21/11/18 19:21:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark-3.2.0/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/11/18 19:21:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setAppName('myApp').setMaster('local[*]')\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345aeeb0-232b-48c9-836f-7f5a763eba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.setLogLevel('WARN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ac5db6-129e-491d-bb64-4cf88b1fbeda",
   "metadata": {},
   "source": [
    "Con este objeto, podemos acceder a la SparkUI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65ad7b63-6a24-4082-891c-8c92a4902052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.43:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>myApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=myApp>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb21526e-ac63-40a9-811f-30c955a3da66",
   "metadata": {},
   "source": [
    "Y consultar el resto de configuraciones por defecto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44df61b9-fd70-4370-aba4-84009ec285d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.port', '36985'),\n",
       " ('spark.app.name', 'myApp'),\n",
       " ('spark.app.id', 'local-1637259671044'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.app.startTime', '1637259669891'),\n",
       " ('spark.driver.host', '192.168.1.43'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2641d514-ecae-4cd3-b2a9-75577e2efac3",
   "metadata": {},
   "source": [
    "### 1.2. RDD\n",
    "\n",
    "Spark gira en torno al concepto de *Resilient Distributed Dataset* (RDD), que es una colección de elementos que se pueden operar en paralelo. Hay dos formas de crear un RDD: paralelizar una colección existente, o hacer referencia a un conjunto de datos en un sistema de almacenamiento externo, como un sistema de archivos compartido, HDFS, HBase o cualquier fuente de datos que ofrezca un formato de entrada Hadoop.\n",
    "\n",
    "Para crear un RDD se utiliza el método `parallelize` del objeto SparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1cc06ae-e2f1-4195-945f-b3682abb80ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear un RDD\n",
    "rdd = sc.parallelize(range(10000000))\n",
    "rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d70d865-4308-4a15-b66b-15609d35e30f",
   "metadata": {},
   "source": [
    "Un parámetro importante para los RDD es el número de particiones para distribuir el conjunto de datos. Spark ejecutará una task para cada partición del clúster. Por lo general, se utilizan de 2 a 4 particiones para cada CPU del clúster. Normalmente, Spark infiere la cantidad de particiones automáticamente en función de los recursos. Sin embargo, también puede configurarlo manualmente pasándolo como un segundo parámetro de parallelize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fed3ba8e-0c63-4939-90b1-c42d70851443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ver las particiones de un RDD (valor 4 por defecto)\n",
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d3e9e75-21e7-48dc-855a-d41cbba6b953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reparticionar un RDD\n",
    "rdd_repartitioned = rdd.repartition(8)\n",
    "rdd_repartitioned.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71564235-4e67-40c7-aae1-a985c234c84f",
   "metadata": {},
   "source": [
    "### 1.3. Operaciones con RDD\n",
    "\n",
    "Los RDDs soportan dos tipos de operaciones:\n",
    "\n",
    "- transformaciones\n",
    "- acciones\n",
    "\n",
    "Las transformaciones tienen como consecuencia la creación de un nuevo dataset a partir de uno ya existente, mientras que las acciones, devuelven un valor al driver del cluster después de realizar alguna operación sobre el dataset. La función `map()` por ejemplo sería una transformación, ya que aplica una función a cada valor de un dataset; por otro lado, la función `reduce()` devuelve un único valor fruto de algún tipo de agregación.\n",
    "\n",
    "Las transformaciones en Spark se denominan *lazy*, ya que no se ejecutan hasta que se aplica una acción sobre el dataset. Esta característica permite que Spark se ejecute de una forma más eficiente. Por ejemplo si sobre un dataset realizamos un `map()` y luego un `reduce()`, al driver únicamente se le informará del valor de la acción y no de todos los pasos anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07d33344-d6c8-44b7-bc66-80ab6d15b3da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[6] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo de operaciones con un RDD\n",
    "# ------------------------------------------------------------------------------\n",
    "# Definimos un puntero al fichero\n",
    "lines = sc.textFile(\"ficheros/extracto-quijote.txt\")\n",
    "\n",
    "# Definimos un nuevo RDD sobre el puntero anterior\n",
    "lineLengths = lines.map(lambda s: len(s))\n",
    "\n",
    "lineLengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29ed254-c6ec-4a32-884b-0357f66306db",
   "metadata": {},
   "source": [
    "Si intentamos ver el contenido de lineLengths, veremos que no aparece nada, y solo da información del tipo de objeto que es, aún no se ha realizado ninguna operación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2f4e631-3b28-4965-8856-973d89a95a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Realizamos una acción sobre el RDD\n",
    "totalLength = lineLengths.reduce(lambda a, b: a + b)\n",
    "\n",
    "totalLength"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c92ba6-d331-47cb-8336-fe9642128da1",
   "metadata": {},
   "source": [
    "Finalmente, realizamos una acción sobre el RDD lineLengths. Es en este punto, donde Spark comienza a ejecutar todo el trabajo sobre el RDD, para finalmente enviar un valor al driver del cluster. Si por un casual se fuera a trabajar con el RDD lineLengths de forma repetida, se puede almacenar en memoria mediante el método `persist()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cede6e2d-090e-4e62-b4fc-a5bbf2b9507e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[6] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lineLengths.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f32295-248e-4c49-8f74-99101e1a3cd3",
   "metadata": {},
   "source": [
    "## 2. Bibliografía\n",
    "\n",
    "- https://spark.apache.org/docs/latest/rdd-programming-guide.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
