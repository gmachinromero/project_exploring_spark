{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05efd63c-9661-45cb-baed-50b8705e5c1c",
   "metadata": {},
   "source": [
    "## Primeros pasos con Spark\n",
    "\n",
    "Vamos a utilizar la librería PySpark, que es la API de Python para interactuar con la shell de Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a326aed-85a7-48d6-aa2a-8ce5e8dfa435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías y dependencias\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a52afba-ee27-4045-b27c-56e6f8b71cba",
   "metadata": {},
   "source": [
    "### SparkContext\n",
    "\n",
    "El punto de entrada de cualquier programa PySpark es un objeto SparkContext. Este objeto le indica como conectarse a un clúster de Spark y crear RDDs. La `local[*]` es una cadena especial que indica que está utilizando un clúster local. El * le indica a Spark que cree tantos hilos de trabajo como cores tenga la máquina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62656669-42c3-4891-9be9-4707eeb3ac6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/15 18:40:31 WARN Utils: Your hostname, gmachin resolves to a loopback address: 127.0.1.1; using 192.168.1.43 instead (on interface enp4s0)\n",
      "21/11/15 18:40:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark-3.2.0/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/11/15 18:40:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sc = pyspark.SparkContext('local[*]')\n",
    "# sc.setLogLevel('WARN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ac5db6-129e-491d-bb64-4cf88b1fbeda",
   "metadata": {},
   "source": [
    "Con este objeto, podemos acceder a la SparkUI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65ad7b63-6a24-4082-891c-8c92a4902052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.43:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb21526e-ac63-40a9-811f-30c955a3da66",
   "metadata": {},
   "source": [
    "Y consultar el resto de configuraciones por defecto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44df61b9-fd70-4370-aba4-84009ec285d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.port', '35411'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.app.id', 'local-1636998033814'),\n",
       " ('spark.driver.host', '192.168.1.43'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.app.name', 'pyspark-shell'),\n",
       " ('spark.app.startTime', '1636998032504')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2641d514-ecae-4cd3-b2a9-75577e2efac3",
   "metadata": {},
   "source": [
    "### RDD\n",
    "\n",
    "Spark gira en torno al concepto de *resilient distributed data* (RDD), que es una colección de elementos que se pueden operar en paralelo. Hay dos formas de crear un RDD: paralelizar una colección existente, o hacer referencia a un conjunto de datos en un sistema de almacenamiento externo, como un sistema de archivos compartido, HDFS, HBase o cualquier fuente de datos que ofrezca un formato de entrada Hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1cc06ae-e2f1-4195-945f-b3682abb80ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear un RDD\n",
    "rdd = sc.parallelize(range(10000000))\n",
    "rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d70d865-4308-4a15-b66b-15609d35e30f",
   "metadata": {},
   "source": [
    "Un parámetro importante para los RDD es el número de particiones para distribuir el conjunto de datos. Spark ejecutará una task para cada partición del clúster. Por lo general, se utilizan de 2 a 4 particiones para cada CPU del clúster. Normalmente, Spark infiere la cantidad de particiones automáticamente en función de los recursos. Sin embargo, también puede configurarlo manualmente pasándolo como un segundo parámetro de parallelize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fed3ba8e-0c63-4939-90b1-c42d70851443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ver las particiones de un RDD (valor 4 por defecto)\n",
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d3e9e75-21e7-48dc-855a-d41cbba6b953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reparticionar un RDD\n",
    "rdd_repartitioned = rdd.repartition(8)\n",
    "rdd_repartitioned.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440ad66a-b47c-43cf-9c53-d35814565ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d33344-d6c8-44b7-bc66-80ab6d15b3da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85f32295-248e-4c49-8f74-99101e1a3cd3",
   "metadata": {},
   "source": [
    "## Bibliografía\n",
    "\n",
    "- https://spark.apache.org/docs/latest/rdd-programming-guide.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
